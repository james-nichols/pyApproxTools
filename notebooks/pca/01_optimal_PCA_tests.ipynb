{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\def \\dot #1#2{\\left\\langle #1, #2 \\right\\rangle}$\n",
    "$\\def \\adot #1#2{\\left\\langle #1, #2 \\right\\rangle}$\n",
    "$\\def \\cA {\\mathcal{A}}$\n",
    "$\\def \\cD {\\mathcal{D}}$\n",
    "$\\def \\cM {\\mathcal{M}}$\n",
    "$\\def \\cN {\\mathcal{N}}$\n",
    "$\\def \\cW {\\mathcal{W}}$\n",
    "$\\def \\bc {\\mathbf{c}}$\n",
    "$\\def \\bu {\\mathbf{u}}$\n",
    "$\\def \\bv {\\mathbf{v}}$\n",
    "$\\def \\bw {\\mathbf{w}}$\n",
    "$\\def \\bG {\\mathbf{G}}$\n",
    "$\\def \\bC {\\mathbf{C}}$\n",
    "$\\def \\bD {\\mathbf{D}}$\n",
    "$\\def \\bI {\\mathbf{I}}$\n",
    "$\\def \\bR {\\mathbf{R}}$\n",
    "$\\def \\bS {\\mathbf{S}}$\n",
    "$\\def \\bT {\\mathbf{T}}$\n",
    "$\\def \\bU {\\mathbf{U}}$\n",
    "$\\def \\bV {\\mathbf{V}}$\n",
    "$\\def \\bW {\\mathbf{W}}$\n",
    "$\\def \\bPhi {\\mathbf{\\Phi}}$\n",
    "$\\def \\bPsi {\\mathbf{\\Psi}}$\n",
    "$\\def \\bGamma {\\mathbf{\\Gamma}}$\n",
    "$\\def \\bSigma {\\mathbf{\\Sigma}}$\n",
    "$\\def \\bTheta {\\mathbf{\\Theta}}$\n",
    "$\\def \\bOmega {\\mathbf{\\Omega}}$\n",
    "$\\def \\bbE {\\mathbb{E}}$\n",
    "$\\def \\bbP {\\mathbb{P}}$\n",
    "$\\def \\bbR {\\mathbb{R}}$\n",
    "$\\def \\bbN {\\mathbb{N}}$\n",
    "\n",
    "# What is the optimal PCA reconstruction point when $n < N < K$?\n",
    "\n",
    "Lets consider the ambient space $\\bbR^K$, for some very large $K$ that we'll likely never attain in our number of samples $N$, and in any case we consider the case where $\\cM$ is embedded in a space of dimension no more than $n$. This presents potentially a better setting to calculate our optimal reconstruction, but we lose a bunch of the results used in the document so far. I attempt to re-calculate some of these quantities or demonstrate best analogues.\n",
    "\n",
    "First we'll generate a collection of $N$ points in an $M$ dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import importlib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import pyApproxTools as pat\n",
    "importlib.reload(pat)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=100, n=4, N=20, U is dim (100, 20)\n",
      "Wm is of dimension 6 and beta(W, Phi) is 0.09902624038430155\n"
     ]
    }
   ],
   "source": [
    "K = 100  # The dimensionality of the ambient space (can be up to 2^16 for FEM solutions)\n",
    "n = 4    # The truncation dimension of the PCA / embedding dimension of the manifold \n",
    "N = 20   # Number of dictionary samples (NB Note used yet, only for \"empirical\" results)\n",
    "m = 6    # The dimension off the measurement space\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# First make two random orthonormal vector bases\n",
    "Phi = sp.stats.ortho_group.rvs(dim=K) # The \"PCA\" space\n",
    "Psi = sp.stats.ortho_group.rvs(dim=K) # The \"measurement\" space\n",
    "\n",
    "sigma = np.sort(np.random.random(K))[::-1]\n",
    "#sigma[n:] = 0\n",
    "D = np.diag(sigma**2)\n",
    "D_inv = np.diag(1.0 / sigma**2)\n",
    "Sigma = np.diag(sigma[:n])\n",
    "Sigma_inv = np.diag(1.0 / sigma[:n])\n",
    "\n",
    "# This is the original covariance matrix!\n",
    "u0 = np.random.random(K) / np.sqrt(K) # u_0 is the notional center\n",
    "V = Phi[:,:n]\n",
    "C = V @ Sigma @ Sigma @ V.T\n",
    "points = np.random.multivariate_normal(u0, C, N)\n",
    "\n",
    "u0_e = points.mean(axis=0) # Empirical mean...\n",
    "U = (points - u0_e).T\n",
    "print('K={0}, n={1}, N={2}, U is dim {3}'.format(K,n,N,U.shape))\n",
    "\n",
    "W = Psi[:,:m]\n",
    "CG = W.T @ Phi[:,:n]\n",
    "betas = sp.linalg.svd(CG, compute_uv=False)\n",
    "\n",
    "print('Wm is of dimension {0} and beta(W, Phi) is {1}'.format(m, betas[-1] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_e is of shape (100, 100) but only of rank 4\n",
      "G_e is of shape (20, 20) but only of rank 4\n"
     ]
    }
   ],
   "source": [
    "G_e = U.T @ U\n",
    "C_e = U @ U.T\n",
    "\n",
    "print('C_e is of shape {0} but only of rank {1}'.format(C_e.shape, np.linalg.matrix_rank(C_e)))\n",
    "print('G_e is of shape {0} but only of rank {1}'.format(G_e.shape, np.linalg.matrix_rank(G_e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in the above code we write ```C_e``` to mean \"empirical\" $\\bC$.\n",
    "\n",
    "Now we test the two ways to calculate the best \"estimator\" for some $u$ and corresponding measurement $u$. __Let us forget the complications of the \"empirical\" vs \"true\" PCA for now__, and just use the two orthonormal bases $\\bPhi = [\\varphi_1,\\ldots,\\varphi_K]$, with associated $\\sigma_1,\\ldots,\\sigma_K$, and $\\bPsi = [\\psi_1,\\ldots,\\psi_K]$ for which $\\bW = [\\psi_1,\\ldots,\\psi_m]$ is the measurement basis, obviously $\\bW_\\perp = [\\psi_{m+1},\\ldots,\\psi_K]$.\n",
    "\n",
    "First we generate a point $u$ that is not too far from $u_0$, and in the span of $[\\varphi_1,\\ldots,\\varphi_n]$, and take its measurement in $\\bW$, given here by $P_W u = \\bW \\bW^T u$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| u ||       = 0.6151958492557501\n",
      "|| u_0 ||     = 0.6151351606403517\n",
      "|| u - u_0 || = 0.003907277301185513\n",
      "|| w ||       = 0.17874357801488\n",
      "|| u - w ||   = 0.5886566624611856\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "pert = (1e-2 * np.random.random(n) / np.sqrt(n) * sigma[:n])\n",
    "\n",
    "u = u0 + Phi[:,:n] @ pert\n",
    "w = W @ W.T @ u\n",
    "\n",
    "print('|| u ||       =', np.linalg.norm(u))\n",
    "print('|| u_0 ||     =', np.linalg.norm(u0))\n",
    "print('|| u - u_0 || =', np.linalg.norm(u - u0))\n",
    "print('|| w ||       =', np.linalg.norm(w))\n",
    "print('|| u - w ||   =', np.linalg.norm(u - w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Albert's scheme is the following:\n",
    "\n",
    "$$\n",
    "\\bw_\\perp^* = -\\bT_{2,2}^{-1} \\bT_{2,1} \\bw\n",
    "$$\n",
    "\n",
    "where $\\bw_\\perp^*\\in\\bbR^{K-m}$ and $\\bw\\in\\bbR^m$, i.e. they are the representation in the $\\psi_i$ basis.\n",
    "\n",
    "where $\\bT = \\bG^T \\bD^{-1} \\bG = (\\bPhi^T \\bPsi)^T \\bD^{-1} (\\bPhi^T \\bPsi)$, the diagonal matrix $\\bD = \\mathrm{diag}(\\sigma_1^2,\\ldots,\\sigma_n^2)$, and (2,2) subscript refers to the first $m$ or second $m$ block.\n",
    "\n",
    "Note also that, using the above notation $\\bPsi = \\begin{bmatrix} \\bW & \\bW_\\perp \\end{bmatrix}$, so in fact\n",
    "\n",
    "$$\n",
    "\\bG = \\bPhi^T \\bPsi = \\begin{bmatrix} \\bPhi^T\\bW & \\bPhi^T\\bW_\\perp \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Writing $\\bSigma_K = \\mathrm{diag}(\\sigma_1^2,\\ldots,\\sigma_K^2)$, i.e. not truncated at $n$, we see that we get\n",
    "\n",
    "$$\n",
    "\\bT = \n",
    "\\begin{bmatrix}\n",
    "\\bW^T\\bPhi \\\\\n",
    "\\bW_\\perp^T\\bPhi\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\\bSigma_K^{-1} \\end{bmatrix}\n",
    "\\begin{bmatrix} \\bPhi^T \\bW & \\bPhi^T \\bW_\\perp \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\bW^T \\bPhi \\bSigma_K^{-1} \\bPhi^T \\bW & \\bW^T \\bPhi \\bSigma_K^{-1} \\bPhi^T \\bW_\\perp \\\\\n",
    "\\bW_\\perp^T \\bPhi \\bSigma_K^{-1} \\bPhi^T \\bW & \\bW_\\perp^T \\bPhi \\bSigma_K^{-1} \\bPhi^T \\bW_\\perp\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which give us the expressions for $\\bT_{1,1}$, $\\bT_{1,2}$ etc...\n",
    "\n",
    "In fact, another way to approach this whole problem see it as the Schur complement problem. If $\\bu \\in \\bbR^K$ is $u$ described in the $\\psi_i$ basis, then the minimisation is looking at \n",
    "\n",
    "$$\n",
    "\\bu^* =\n",
    "\\mathrm{argmin}_{P_W \\bu = \\bw} \\bu^T \\bT \\bu = \\mathrm{argmin}_{\\bw_\\perp^*} \n",
    "\\begin{bmatrix} \\bw \\\\ \\bw_\\perp \\end{bmatrix}^T  \n",
    "\\begin{bmatrix}\n",
    "\\bT_{1,1} & \\bT_{1,2} \\\\\n",
    "\\bT_{2,1} & \\bT_{2,2} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \\bw \\\\ \\bw_\\perp \\end{bmatrix}\n",
    "$$\n",
    "which can eventually be found \n",
    "$$\n",
    "\\bw_\\perp^* =\n",
    "\\mathrm{argmin}_{\\bw_\\perp} \n",
    "\\left( \\bw^T \\bT_{1,1} \\bw^T + 2 \\bw_\\perp^T \\bT_{2,1} \\bw + \\bw_\\perp^T \\bT_{2,2} \\bw_\\perp \\right)\n",
    "$$\n",
    "\n",
    "Just the old quadratic form in terms of $\\bw_\\perp$, and the minimiser is $\\bT_{2,1} \\bw + \\bT_{2,2} \\bw_\\perp^* = 0$ and we get the solution above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank: 94 (94, 94)\n",
      "|| T_21 - Psi[:,m:].T @ Phi @ D_inv @ Phi.T @ Psi[:,:m] ||_F = 1.2267966862714528e-11 (Frobenius norm)\n",
      "\n",
      "Condition numbers: \n",
      "kappa(T_22) = 78211.85097357862\n",
      "kappa(T_21) = 265.7405087857801\n",
      "\n",
      "|| u - u* ||       = 0.588993579389868\n",
      "|| u_0 - u* ||     = 0.588915257111193\n",
      "|| w - u* ||       = 0.1257189368383046\n"
     ]
    }
   ],
   "source": [
    "G = (Phi.T @ Psi)\n",
    "T = G.T @ D_inv @ G\n",
    "\n",
    "T21 = T[m:, :m]\n",
    "T22 = T[m:, m:]\n",
    "\n",
    "#T21 = Psi[:,m:].T @ V @ np.diag(1.0/sigma[:n]**2) @ V.T @ Psi[:,:m]\n",
    "#T22 = Psi[:,m:].T @ V @ np.diag(1.0/sigma[:n]**2) @ V.T @ Psi[:,m:]\n",
    "print('rank:', np.linalg.matrix_rank(T22), T22.shape)\n",
    "# Just to check\n",
    "print('|| T_21 - Psi[:,m:].T @ Phi @ D_inv @ Phi.T @ Psi[:,:m] ||_F \\\n",
    "= {0} (Frobenius norm)'.format(np.linalg.norm(T21 - Psi[:,m:].T @ Phi @ D_inv @ Phi.T @ Psi[:,:m] )))\n",
    "\n",
    "print('\\nCondition numbers: \\nkappa(T_22) = {0}\\nkappa(T_21) = {1}\\n'.format(np.linalg.cond(T22), np.linalg.cond(T21)))\n",
    "\n",
    "#w_perp_star = - np.linalg.solve(T22, T21 @ W.T @ w)\n",
    "w_perp_star = - np.linalg.pinv(T22) @ T21 @ W.T @ w\n",
    "\n",
    "u_star_albert = w + Psi[:,m:] @ w_perp_star\n",
    "\n",
    "print('|| u - u* ||       =', np.linalg.norm(u - u_star_albert))\n",
    "print('|| u_0 - u* ||     =', np.linalg.norm(u0 - u_star_albert))\n",
    "print('|| w - u* ||       =', np.linalg.norm(w - u_star_albert))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets test a few aspects of this\n",
    "\n",
    " - Do we have that $\\cA(u^*) = u^*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| u* - u** ||       = 4.4576075707668593e-13\n"
     ]
    }
   ],
   "source": [
    "w_u_star = W @ W.T @ u_star_albert\n",
    "w_perp_star_star = - np.linalg.solve(T22, T21 @ W.T @ w_u_star)\n",
    "u_star_star = w + Psi[:,m:] @ w_perp_star_star\n",
    "print('|| u* - u** ||       =', np.linalg.norm(u_star_star - u_star_albert))\n",
    "\n",
    "#u_star_coords = (Phi.T @ (u_star_albert - u0))\n",
    "#print(u_star_coords)\n",
    "#print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop press, try Lagrange multipliers\n",
    "\n",
    "We wan't to minimise $\\tilde u^T \\tilde u$ given that $\\bW^T \\bV \\bSigma \\tilde u = \\bW^T w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another scheme is to fix $W_\\perp$ within $V$\n",
    "\n",
    "How does this work? My rationalle is that $u^*$ is a unique minimiser of the intersection of $W_\\perp + w$ and $u_0 + V_n$. _Surely_ we can express this just in terms of $V_n$?\n",
    "\n",
    "So - given $u \\in \\bbR^K$, when is expressed in the $\\varphi_i$ coordinates, we have that\n",
    "\n",
    "$$u = \\bV u_V$$\n",
    "\n",
    "and $\\bW^T \\bV u_V = \\bW^T w$, obv we have that $\\bW^T \\bV \\in \\bbR^{m \\times n}$, so has rank at most $n$. Ah, this is where the lifting business is important - because we know that \n",
    "\n",
    "Now, a rank $K-m$ problem is\n",
    "$$\n",
    "\\bW \\bW^T u = w\n",
    "$$\n",
    "\n",
    "My logic is as follows: for any $\\tilde u \\in \\bbR^n$ where $u\\in V_n$ is expressed by $u = \\bV \\bSigma \\tilde u$ we are doing the minimisation with the following quantity fixed.\n",
    "\n",
    "$$\n",
    "w = \\bW \\bW^T \\bV \\bSigma \\tilde u\n",
    "$$\n",
    "\n",
    "Also note that \n",
    "\n",
    "$$\n",
    "\\bW \\bW^T + \\bW_\\perp \\bW_\\perp^T = \\bPhi \\bPhi^T = I\n",
    "$$\n",
    "\n",
    "So \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "u \n",
    "&= w + w_\\perp \\\\\n",
    "&= \\bW \\bW^T \\bV \\bSigma \\tilde u + \\bW_\\perp \\bW_\\perp^T \\bV \\bSigma \\tilde u \\\\\n",
    "&= (\\bW \\bW^T \\bV+ (I - \\bW \\bW^T) ) \\bV \\bSigma \\tilde u \\\\\n",
    "&= w + (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can write $u$ as \n",
    "\n",
    "So we want to minimise $\\tilde u^t \\tilde u$ such that $\\bW \\bW^T u = \\bW \\bW^T \\bV \\bSigma \\tilde u = w$, well then we want to minimise\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\left( \\bSigma^{-1} \\bV^T (w + w_\\perp) \\right)^T \\left( \\bSigma^{-1} \\bV^T (w + w_\\perp)\\right) \n",
    "& = w^T \\bV \\bSigma^{-2} \\bV^T w + 2 w^T \\bV \\bSigma^{-2} \\bV^T w_\\perp + w_\\perp^T \\bV \\bSigma^{-2} \\bV^T w_\\perp \\\\\n",
    "& = w^T \\bV \\bSigma^{-2} \\bV^T w + 2 w^T \\bV \\bSigma^{-2} \\bV^T (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u + \\left( (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u \\right)^T \\bV \\bSigma^{-2} \\bV^T (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we want to take the derivative in the direction of $\\bSigma^{-1} \\bV^T (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 (6, 4)\n",
      "[-0.00270544  0.00576414  0.00821772 -0.02313032]\n",
      "[-0.00270544  0.00576414  0.00821772 -0.02313032]\n",
      "[-0.02466493  0.01824618  0.02793696 -0.00497143  0.0193807   0.01769877\n",
      " -0.02983778  0.02728139  0.03505706 -0.01270915  0.02442923 -0.01069332\n",
      "  0.02690723  0.00376965  0.01954816  0.01289991 -0.00215582  0.02551497\n",
      " -0.0396289   0.01490916  0.00439514  0.01893165  0.01204279  0.01101446\n",
      "  0.02982631  0.01635514 -0.00336866 -0.01909542  0.00465681  0.01459496\n",
      " -0.02171893  0.04871353 -0.03570855 -0.01749501 -0.03866849 -0.01598471\n",
      " -0.01264606  0.01050698 -0.01747766 -0.00996862 -0.00056229 -0.01564142\n",
      "  0.01466246  0.03424158 -0.04256694  0.02240546  0.00686161  0.00305633\n",
      "  0.01918888 -0.0053641   0.02021473 -0.00502451 -0.02997484  0.05562247\n",
      "  0.00496557  0.02945244  0.00862564  0.01381963  0.01165388  0.01489417\n",
      "  0.00496655  0.03752073  0.00286693 -0.01800639  0.04572924  0.017934\n",
      " -0.02111822  0.00049498  0.00954262  0.04176328 -0.00190204 -0.01054928\n",
      "  0.00493412  0.03621019  0.00044377  0.001206   -0.00215146  0.04195959\n",
      " -0.0050688   0.01382738  0.00094679  0.03791065 -0.04542096  0.00759022\n",
      "  0.0109645   0.01911364 -0.01552926 -0.00541963  0.02229737  0.01506915\n",
      " -0.04052223  0.03001987  0.02540844 -0.00670183 -0.00977515 -0.00731563\n",
      " -0.05164926  0.01901915  0.02744738  0.0153668 ]\n",
      "\n",
      "|| u*_a - u*_1 ||   = 0.21750892852090462\n",
      "\n",
      "|| u*_a - u*_2 ||   = 0.22224847594162914\n",
      "\n",
      "|| u*_1 - u*_2 ||   = 0.04806870731945401\n"
     ]
    }
   ],
   "source": [
    "G = W.T @ V @ Sigma\n",
    "print(np.linalg.matrix_rank(G), G.shape)\n",
    "\n",
    "uG, sG, vG = sp.linalg.svd(G)\n",
    "\n",
    "Wfav = W @ uG.T\n",
    "Vfav = V @ vG.T\n",
    "\n",
    "#print(Wfav.shape, Vfav.shape)\n",
    "#print(Wfav[:,:n] @ Wfav[:,:n].T @ (u-u0) - W @ W.T @ (u-u0))\n",
    "\n",
    "# Ok - so Wfav up til vector n will definitely contain w, and further more be of the same dimensionality\n",
    "lam_tilde = (1.0/(sG*sG)) * (Wfav[:,:n].T @ (u-u0))\n",
    "lam_tilde_2 = (1.0/(sG*sG)) * (uG[:n] @ (W.T @ (u-u0)))\n",
    "\n",
    "print(lam_tilde)\n",
    "\n",
    "print(lam_tilde_2)\n",
    "\n",
    "u_t_a = u_star_albert - Vfav @ Vfav.T @ (u_star_albert - u0)\n",
    "print(u_t_a)\n",
    "\n",
    "u_star_1 = Vfav @ lam_tilde_2\n",
    "u_star_2 = V @ Sigma @ u_tilde\n",
    "\n",
    "print('\\n|| u*_a - u*_1 ||   =', np.linalg.norm(u_star_albert - u_star_1))\n",
    "print('\\n|| u*_a - u*_2 ||   =', np.linalg.norm(u_star_albert - u_star_2))\n",
    "print('\\n|| u*_1 - u*_2 ||   =', np.linalg.norm(u_star_1 - u_star_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 (100, 100)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (100,) (4,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-324-7ba68af75fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Ok - so Wfav up til vector n will definitely contain w, and further more be of the same dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlam_tilde\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msQ\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mPsifav\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mu0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mlam_tilde_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msQ\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mPsi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mu0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (100,) (4,) "
     ]
    }
   ],
   "source": [
    "Q = Psi.T @ Phi @ Sigma_K\n",
    "print(np.linalg.matrix_rank(Q), Q.shape)\n",
    "\n",
    "uQ, sQ, vQ = sp.linalg.svd(Q)\n",
    "\n",
    "Psifav = Psi @ uQ.T\n",
    "Phifav = Phi @ vQ.T\n",
    "\n",
    "# Ok - so Wfav up til vector n will definitely contain w, and further more be of the same dimensionality\n",
    "lam_tilde = (1.0/(sQ*sQ)) * (Psifav[:,:n].T @ (u-u0))\n",
    "lam_tilde_2 = (1.0/(sQ*sQ)) * (uQ[:n] @ (Psi.T @ (u-u0)))\n",
    "\n",
    "print(lam_tilde)\n",
    "\n",
    "print(lam_tilde_2)\n",
    "\n",
    "u_t_a = u_star_albert - Vfav @ Vfasv.T @ (u_star_albert - u0)\n",
    "print(u_t_a)\n",
    "\n",
    "u_star_1 = Vfav @ lam_tilde_2\n",
    "u_star_2 = V @ Sigma @ u_tilde\n",
    "\n",
    "print('\\n|| u*_a - u*_1 ||   =', np.linalg.norm(u_star_albert - u_star_1))\n",
    "print('\\n|| u*_a - u*_2 ||   =', np.linalg.norm(u_star_albert - u_star_2))\n",
    "print('\\n|| u*_1 - u*_2 ||   =', np.linalg.norm(u_star_1 - u_star_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 (6, 6)\n",
      "[1.31420142e-01 4.04876648e-02 3.34379060e-02 8.77059406e-03\n",
      " 1.47067581e-17 5.90966432e-18]\n",
      "8 (10, 10)\n",
      "[[-0.55346245  0.07233685 -0.47077718  0.68323901]\n",
      " [ 0.37343352  0.10942025  0.59652722  0.70194721]\n",
      " [-0.13476957 -0.97477915  0.14921115  0.09684423]\n",
      " [-0.73216384  0.18055526  0.63266117 -0.17628319]]\n",
      "[0.99019436 0.9625923  0.95476322 0.85379097]\n",
      "[[-0.55365442  0.3733379  -0.13478242 -0.73206509]\n",
      " [ 0.07237527  0.10945139 -0.9747739   0.18054931]\n",
      " [-0.47075496  0.59646167  0.14921713  0.63273809]\n",
      " [ 0.6830947   0.70204892  0.09686995 -0.17642325]]\n",
      "[[-0.68234186  0.46998488  0.07260746  0.55520443]\n",
      " [-0.70257204 -0.59573491  0.10954409 -0.37348699]\n",
      " [-0.09695443 -0.14938953 -0.97474001  0.13477585]\n",
      " [ 0.17720596 -0.63395339  0.18058286  0.73081528]]\n",
      "[0.14620933 0.04523695 0.03740772 0.00980608]\n",
      "[[-0.68399169 -0.70142145 -0.09675932  0.17550201]\n",
      " [ 0.47154275 -0.59725804 -0.14903873 -0.63144099]\n",
      " [ 0.07210737  0.10932621 -0.9748124   0.18052448]\n",
      " [ 0.5519088  -0.37328081  0.13478083  0.73341137]]\n",
      "14.910076962481174 1.1597620488096787\n"
     ]
    }
   ],
   "source": [
    "#print((W.T @ V @ Sigma).shape, Sigma.shape, V.shape, W.shape)\n",
    "L = np.block([[np.eye(n), -Sigma @ V.T @ W], [W.T @ V @ Sigma, np.zeros((m,m))]])\n",
    "\n",
    "print(np.linalg.matrix_rank(L), L.shape)\n",
    "\n",
    "wL = np.block([np.zeros(n), W.T @ u])\n",
    "\n",
    "wot = np.linalg.solve(L, wL)\n",
    "wot2 = np.linalg.pinv(L) @ wL\n",
    "\n",
    "Q = Sigma_inv @ V.T @ W @ W.T @ V @ Sigma\n",
    "Q_perp = Sigma_inv @ V.T @ (np.eye(K) - W @ W.T) @ V @ Sigma\n",
    "\n",
    "#print(sp.linalg.svd(W.T @ V @ Sigma))\n",
    "print(sp.linalg.svd(Sigma_inv @ V.T @ (np.eye(K) - W @ W.T) @ V @ Sigma)[0])\n",
    "print(sp.linalg.svd(Sigma_inv @ V.T @ (np.eye(K) - W @ W.T) @ V @ Sigma)[1])\n",
    "print(sp.linalg.svd(Sigma_inv @ V.T @ (np.eye(K) - W @ W.T) @ V @ Sigma)[2])\n",
    "print(sp.linalg.svd(Sigma_inv @ V.T @ (W @ W.T) @ V @ Sigma)[0])\n",
    "print(sp.linalg.svd(Sigma_inv @ V.T @ (W @ W.T) @ V @ Sigma)[1])\n",
    "print(sp.linalg.svd(Sigma_inv @ V.T @ (W @ W.T) @ V @ Sigma)[2])\n",
    "#print(np.linalg.norm(V @ wot2[:n] + u0 - u_star_albert))\n",
    "\n",
    "print(np.linalg.cond(Q), np.linalg.cond(Q_perp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W_tilde W_perp_tilde] rank 12\n",
      "[W_tilde] rank 6\n",
      "[W_perp_tilde] rank 6\n",
      "Q is of shape (100, 100) and rank 6\n",
      "Q_perp is of shape (100, 100) and rank 94\n",
      "\n",
      "|| u ||       = 0.6151958492557501\n",
      "|| u_0 ||     = 0.6151351606403517\n",
      "|| u - u_0 || = 0.003907277301185513\n",
      "|| w ||       = 0.17874357801488\n",
      "|| u - w ||   = 0.5886566624611856\n",
      "\n",
      "|| u - u*j ||       = 0.012489005066512555\n",
      "|| u_0 - u*j ||     = 0.011953059143353355\n",
      "|| w - u*j ||       = 0.5906234073338432\n",
      "|| u*_j - u**_j ||  = 4.065974981473035e-17\n",
      "\n",
      "|| u - u*a ||       = 0.588993579389868\n",
      "|| u_0 - u*a ||     = 0.5889152571111931\n",
      "|| w - u*a ||       = 0.1257189368383046\n",
      "\n",
      "|| u*_a - u*_j ||   = 0.5913580920764393\n",
      "\n",
      "||C u - u*j ||       = 6.81501929565927e-05\n",
      "||C u - u*a ||       = 0.12814025702631107\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "w_c = w - W @ W.T @ u0\n",
    "w_c_w = W.T @ (u - u0)\n",
    "\n",
    "Sigma_inv_K = np.diag(1.0/sigma)\n",
    "Sigma_K = np.diag(sigma)\n",
    "Q = Sigma_inv_K @ Phi.T @ W @ W.T @ Phi @ Sigma_K\n",
    "Q_perp = Sigma_inv_K @ Phi.T @ (np.eye(K) - W @ W.T) @ Phi @ Sigma_K\n",
    "\n",
    "#Q = np.diag(1.0/sigma[:n]) @ V.T @ W @ W.T @ V @ np.diag(sigma[:n])\n",
    "#Q_perp = np.diag(1.0/sigma[:n]) @ V.T @ (np.eye(K) - W @ W.T) @ V @ np.diag(sigma[:n])\n",
    "\n",
    "#Q = W @ W.T @ Phi @ Sigma_K\n",
    "#Q_perp = (np.eye(K) - W @ W.T) @ Phi @ Sigma_K\n",
    "#u_star_tilde_james = np.linalg.solve(Q_perp, w_c)\n",
    "#u_star_james = V @ Sigma @ u_star_tilde_james\n",
    "\n",
    "uQ,sQ,vQh = sp.linalg.svd(Q)\n",
    "uQp,sQp,vQph = sp.linalg.svd(Q_perp)\n",
    "\n",
    "W_perp_tilde_perp = vQph[sQp < 1e-12]\n",
    "W_tilde = vQh[sQ > 1e-12] # We could also find W_tilde_perp = vQh[sQp < 1e-12]\n",
    "\n",
    "# The solution *should* be unique in the span of W_perp_tilde_perp, which means\n",
    "u_star_james = u0 + W_perp_tilde_perp.T @ np.linalg.solve(W.T @ W_perp_tilde_perp.T, W.T @ (u - u0))\n",
    "\n",
    "# NOTE THAT THESE TWO ARE NOT ORTHOGONAL!!!\n",
    "print('[W_tilde W_perp_tilde] rank {0}'.format(np.linalg.matrix_rank(np.vstack((W_tilde, W_perp_tilde)))))\n",
    "print('[W_tilde] rank {0}'.format(np.linalg.matrix_rank(W_tilde)))\n",
    "print('[W_perp_tilde] rank {0}'.format(np.linalg.matrix_rank(np.vstack(W_perp_tilde))))\n",
    "\n",
    "print('Q is of shape {0} and rank {1}'.format(Q.shape, np.linalg.matrix_rank(Q)))\n",
    "print('Q_perp is of shape {0} and rank {1}\\n'.format(Q_perp.shape, np.linalg.matrix_rank(Q_perp)))\n",
    "\n",
    "# Is Albert's answer in the span of W_perp_tilde?\n",
    "#coeffs = W_perp_tilde @ (u_star_albert - u0)\n",
    "#print(coeffs)\n",
    "#print((u_star_albert-u0) - W_tilde.T @ coeffs)\n",
    "\n",
    "# So my answer is *maybe* in the span of W_perp_tilde...\n",
    "\n",
    "# What is going on here?\n",
    "\n",
    "w_c = W.T @ (u_star_james-u0)\n",
    "me = np.linalg.solve(W.T @ W_perp_tilde_perp.T, W.T @ (u_star_james - u0))\n",
    "u_star_star = u0 + W_perp_tilde_perp.T @ me\n",
    "\n",
    "#u_star_tilde_james = np.linalg.solve(Q_perp, Sigma_inv_K @ Phi.T @ w)\n",
    "#u_star_james = Phi @ Sigma_K @ u_star_tilde_james\n",
    "\n",
    "# Can test: explicitly construct Q_perp from W_perp, does indeed give us the same answer\n",
    "#Q_perp_explicit = Sigma_inv @ V.T @ Psi[:,m:] @ Psi[:,m:].T @ V @ Sigma\n",
    "\n",
    "print('|| u ||       =', np.linalg.norm(u))\n",
    "print('|| u_0 ||     =', np.linalg.norm(u0))\n",
    "print('|| u - u_0 || =', np.linalg.norm(u - u0))\n",
    "print('|| w ||       =', np.linalg.norm(w))\n",
    "print('|| u - w ||   =', np.linalg.norm(u - w))\n",
    "print('')\n",
    "print('|| u - u*j ||       =', np.linalg.norm(u - u_star_james))\n",
    "print('|| u_0 - u*j ||     =', np.linalg.norm(u0 - u_star_james))\n",
    "print('|| w - u*j ||       =', np.linalg.norm(w - u_star_james))\n",
    "print('|| u*_j - u**_j ||  =', np.linalg.norm(u_star_james - u_star_star))\n",
    "print('')\n",
    "print('|| u - u*a ||       =', np.linalg.norm(u - u_star_albert))\n",
    "print('|| u_0 - u*a ||     =', np.linalg.norm(u0 - u_star_albert))\n",
    "print('|| w - u*a ||       =', np.linalg.norm(w - u_star_albert))\n",
    "\n",
    "print('\\n|| u*_a - u*_j ||   =', np.linalg.norm(u_star_albert - u_star_james))\n",
    "\n",
    "print('\\n||C u - u*j ||       =', (u-u_star_james).T @ Phi @ D @ Phi.T @ (u-u_star_james))\n",
    "print('||C u - u*a ||       =', (u-u_star_albert).T @ Phi @ D @ Phi.T @ (u-u_star_albert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch\n",
    "$$\n",
    "\\left( \\bSigma^{-1} \\bV (w + (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u) \\right)^T \\left( \\bSigma^{-1} \\bV (w + (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "(w + (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u)^T (w + (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u)\n",
    "= w^T w + 2 ((I - \\bW \\bW^T) \\bV \\bSigma \\tilde u)^T w + ((I - \\bW \\bW^T) \\bV \\bSigma \\tilde u)^T (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u\n",
    "$$\n",
    "\n",
    "worth thinking about:\n",
    " - A favorable basis between $W$ and $V$, maybe with $\\Sigma$ included somehow, see if that yields simpler derivations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch\n",
    "$$\n",
    "\\left( \\bSigma^{-1} \\bV (w + (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u) \\right)^T \\left( \\bSigma^{-1} \\bV (w + (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "(w + (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u)^T (w + (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u)\n",
    "= w^T w + 2 ((I - \\bW \\bW^T) \\bV \\bSigma \\tilde u)^T w + ((I - \\bW \\bW^T) \\bV \\bSigma \\tilde u)^T (I - \\bW \\bW^T) \\bV \\bSigma \\tilde u\n",
    "$$\n",
    "\n",
    "worth thinking about:\n",
    " - A favorable basis between $W$ and $V$, maybe with $\\Sigma$ included somehow, see if that yields simpler derivations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W_tilde W_perp_tilde] rank 12\n",
      "[W_tilde] rank 6\n",
      "[W_perp_tilde] rank 6\n",
      "Q is of shape (100, 100) and rank 6\n",
      "Q_perp is of shape (100, 100) and rank 94\n",
      "\n",
      "|| u ||       = 0.6151958492557501\n",
      "|| u_0 ||     = 0.6151351606403517\n",
      "|| u - u_0 || = 0.003907277301185513\n",
      "|| w ||       = 0.17874357801488\n",
      "|| u - w ||   = 0.5886566624611856\n",
      "\n",
      "|| u - u*j ||       = 0.012489005066512557\n",
      "|| u_0 - u*j ||     = 0.011953059143353355\n",
      "|| w - u*j ||       = 0.5906234073338432\n",
      "|| u*_j - u**_j ||  = 4.065974981473035e-17\n",
      "\n",
      "|| u - u*a ||       = 0.588993579389868\n",
      "|| u_0 - u*a ||     = 0.588915257111193\n",
      "|| w - u*a ||       = 0.1257189368383046\n",
      "\n",
      "|| u*_a - u*_j ||   = 0.5913580920764393\n",
      "\n",
      "||C u - u*j ||       = 6.815019295659268e-05\n",
      "||C u - u*a ||       = 0.12814025702631107\n"
     ]
    }
   ],
   "source": [
    "w_c = w - W @ W.T @ u0\n",
    "w_c_w = W.T @ (u - u0)\n",
    "\n",
    "Sigma_inv_K = np.diag(1.0/sigma)\n",
    "Sigma_K = np.diag(sigma)\n",
    "Q = Sigma_inv_K @ Phi.T @ W @ W.T @ Phi @ Sigma_K\n",
    "Q_perp = Sigma_inv_K @ Phi.T @ (np.eye(K) - W @ W.T) @ Phi @ Sigma_K\n",
    "\n",
    "#Q = np.diag(1.0/sigma[:n]) @ V.T @ W @ W.T @ V @ np.diag(sigma[:n])\n",
    "#Q_perp = np.diag(1.0/sigma[:n]) @ V.T @ (np.eye(K) - W @ W.T) @ V @ np.diag(sigma[:n])\n",
    "\n",
    "#Q = W @ W.T @ Phi @ Sigma_K\n",
    "#Q_perp = (np.eye(K) - W @ W.T) @ Phi @ Sigma_K\n",
    "#u_star_tilde_james = np.linalg.solve(Q_perp, w_c)\n",
    "#u_star_james = V @ Sigma @ u_star_tilde_james\n",
    "\n",
    "uQ,sQ,vQh = sp.linalg.svd(Q)\n",
    "uQp,sQp,vQph = sp.linalg.svd(Q_perp)\n",
    "\n",
    "W_perp_tilde_perp = vQph[sQp < 1e-12]\n",
    "W_tilde = vQh[sQ > 1e-12] # We could also find W_tilde_perp = vQh[sQp < 1e-12]\n",
    "\n",
    "# The solution *should* be unique in the span of W_perp_tilde_perp, which means\n",
    "u_star_james = u0 + W_perp_tilde_perp.T @ np.linalg.solve(W.T @ W_perp_tilde_perp.T, W.T @ (u - u0))\n",
    "\n",
    "# NOTE THAT THESE TWO ARE NOT ORTHOGONAL!!!\n",
    "print('[W_tilde W_perp_tilde] rank {0}'.format(np.linalg.matrix_rank(np.vstack((W_tilde, W_perp_tilde)))))\n",
    "print('[W_tilde] rank {0}'.format(np.linalg.matrix_rank(W_tilde)))\n",
    "print('[W_perp_tilde] rank {0}'.format(np.linalg.matrix_rank(np.vstack(W_perp_tilde))))\n",
    "\n",
    "print('Q is of shape {0} and rank {1}'.format(Q.shape, np.linalg.matrix_rank(Q)))\n",
    "print('Q_perp is of shape {0} and rank {1}\\n'.format(Q_perp.shape, np.linalg.matrix_rank(Q_perp)))\n",
    "\n",
    "# Is Albert's answer in the span of W_perp_tilde?\n",
    "#coeffs = W_perp_tilde @ (u_star_albert - u0)\n",
    "#print(coeffs)\n",
    "#print((u_star_albert-u0) - W_tilde.T @ coeffs)\n",
    "\n",
    "# So my answer is *maybe* in the span of W_perp_tilde...\n",
    "\n",
    "# What is going on here?\n",
    "\n",
    "w_c = W.T @ (u_star_james-u0)\n",
    "me = np.linalg.solve(W.T @ W_perp_tilde_perp.T, W.T @ (u_star_james - u0))\n",
    "u_star_star = u0 + W_perp_tilde_perp.T @ me\n",
    "\n",
    "#u_star_tilde_james = np.linalg.solve(Q_perp, Sigma_inv_K @ Phi.T @ w)\n",
    "#u_star_james = Phi @ Sigma_K @ u_star_tilde_james\n",
    "\n",
    "# Can test: explicitly construct Q_perp from W_perp, does indeed give us the same answer\n",
    "#Q_perp_explicit = Sigma_inv @ V.T @ Psi[:,m:] @ Psi[:,m:].T @ V @ Sigma\n",
    "\n",
    "print('|| u ||       =', np.linalg.norm(u))\n",
    "print('|| u_0 ||     =', np.linalg.norm(u0))\n",
    "print('|| u - u_0 || =', np.linalg.norm(u - u0))\n",
    "print('|| w ||       =', np.linalg.norm(w))\n",
    "print('|| u - w ||   =', np.linalg.norm(u - w))\n",
    "print('')\n",
    "print('|| u - u*j ||       =', np.linalg.norm(u - u_star_james))\n",
    "print('|| u_0 - u*j ||     =', np.linalg.norm(u0 - u_star_james))\n",
    "print('|| w - u*j ||       =', np.linalg.norm(w - u_star_james))\n",
    "print('|| u*_j - u**_j ||  =', np.linalg.norm(u_star_james - u_star_star))\n",
    "print('')\n",
    "print('|| u - u*a ||       =', np.linalg.norm(u - u_star_albert))\n",
    "print('|| u_0 - u*a ||     =', np.linalg.norm(u0 - u_star_albert))\n",
    "print('|| w - u*a ||       =', np.linalg.norm(w - u_star_albert))\n",
    "\n",
    "print('\\n|| u*_a - u*_j ||   =', np.linalg.norm(u_star_albert - u_star_james))\n",
    "\n",
    "print('\\n||C u - u*j ||       =', (u-u_star_james).T @ Phi @ D @ Phi.T @ (u-u_star_james))\n",
    "print('||C u - u*a ||       =', (u-u_star_albert).T @ Phi @ D @ Phi.T @ (u-u_star_albert))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
